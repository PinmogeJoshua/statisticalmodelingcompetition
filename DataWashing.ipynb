{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入库和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "import json\n",
    "from IPython.display import display, Markdown\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei'] \n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 准备停用词\n",
    "if not os.path.exists('cn_stopwords.txt'):\n",
    "    basic_stopwords = \"\"\"的 了 在 是 我 有 和 就 不 人 都 一个 也 很 到 说 要 去 你 会 着 没有 看 好 自己 这 上 我们\"\"\".split()\n",
    "    with open('stopwords.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('\\n'.join(basic_stopwords))\n",
    "    print(\"已创建基础停用词文件 stopwords.txt\")\n",
    "\n",
    "with open('cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopwords = set([line.strip() for line in f if line.strip()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 数据加载和验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载: raw_data\\阿里健康大药房.xlsx (记录数: 1044)\n",
      "成功加载: raw_data\\福士德.xlsx (记录数: 391)\n",
      "成功加载: raw_data\\耀典.xlsx (记录数: 319)\n",
      "成功加载: raw_data\\hfine.xlsx (记录数: 423)\n",
      "\n",
      "已加载数据集:\n",
      "- 阿里健康大药房: 1044条记录\n",
      "- 福士德: 391条记录\n",
      "- 耀典: 319条记录\n",
      "- hfine: 423条记录\n"
     ]
    }
   ],
   "source": [
    "def load_and_validate_data(file_paths):\n",
    "    \"\"\"加载并验证数据\"\"\"\n",
    "    all_datasets = {}\n",
    "    problematic_files = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # 提取数据集名称\n",
    "            dataset_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            \n",
    "            # 读取Excel文件\n",
    "            df = pd.read_excel(file_path)\n",
    "            \n",
    "            # 检查必要列是否存在\n",
    "            if '字段1' not in df.columns:\n",
    "                problematic_files.append((file_path, f\"缺少'字段1'列\"))\n",
    "                continue\n",
    "                \n",
    "            # 检查数据是否为空\n",
    "            if df.empty:\n",
    "                problematic_files.append((file_path, \"数据为空\"))\n",
    "                continue\n",
    "                \n",
    "            # 添加到数据集\n",
    "            all_datasets[dataset_name] = df\n",
    "            print(f\"成功加载: {file_path} (记录数: {len(df)})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            problematic_files.append((file_path, f\"加载错误: {str(e)}\"))\n",
    "    \n",
    "    # 打印问题文件信息\n",
    "    if problematic_files:\n",
    "        print(\"\\n问题文件:\")\n",
    "        for file, reason in problematic_files:\n",
    "            print(f\"- {file}: {reason}\")\n",
    "    \n",
    "    return all_datasets\n",
    "\n",
    "# 文件路径设置\n",
    "file_paths = [\n",
    "    'raw_data\\阿里健康大药房.xlsx',\n",
    "    'raw_data\\福士德.xlsx', \n",
    "    'raw_data\\耀典.xlsx',\n",
    "    'raw_data\\hfine.xlsx'\n",
    "]\n",
    "\n",
    "# 加载数据\n",
    "all_datasets = load_and_validate_data(file_paths)\n",
    "\n",
    "# 显示加载的数据集信息\n",
    "if all_datasets:\n",
    "    print(\"\\n已加载数据集:\")\n",
    "    for name, df in all_datasets.items():\n",
    "        print(f\"- {name}: {len(df)}条记录\")\n",
    "else:\n",
    "    print(\"没有有效数据集被加载\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 分数据集去重函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 新增导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_dataset(df, dataset_name):\n",
    "    \"\"\"对单个数据集执行去重\"\"\"\n",
    "    print(f\"\\n=== 正在处理数据集: {dataset_name} ===\")\n",
    "    print(f\"原始记录数: {len(df)}\")\n",
    "    \n",
    "    # 1. 完全重复去重\n",
    "    df = df.drop_duplicates(subset=['字段1'], keep='first')\n",
    "    print(f\"完全去重后: {len(df)} 条\")\n",
    "    \n",
    "    # 2. 无效评论过滤\n",
    "    invalid_patterns = ['嘻嘻', '哈哈', '好评', '。。。', '！！', r'\\?\\?', r'\\*\\*', '路过', '看看']\n",
    "    \n",
    "    # 构建正则表达式模式，确保特殊字符被转义\n",
    "    pattern = '|'.join([re.escape(p) for p in invalid_patterns])\n",
    "    \n",
    "    # 确保'字段1'是字符串类型\n",
    "    df['字段1'] = df['字段1'].astype(str)\n",
    "    \n",
    "    mask = (\n",
    "        (df['字段1'].str.len() >= 5) &  # 长度大于等于5\n",
    "        ~df['字段1'].str.contains(pattern, na=False, regex=True)  # 不包含无效模式\n",
    "    )\n",
    "    df = df[mask].copy()\n",
    "    print(f\"无效过滤后: {len(df)} 条\")\n",
    "    \n",
    "    # 3. 相似评论去重（仅对小于500条的数据集执行）\n",
    "    if len(df) > 0 and len(df) <= 500:\n",
    "        try:\n",
    "            texts = df['字段1'].tolist()\n",
    "            vectorizer = TfidfVectorizer(tokenizer=jieba.cut, stop_words=list(stopwords))\n",
    "            tfidf = vectorizer.fit_transform(texts)\n",
    "            cos_sim = cosine_similarity(tfidf)\n",
    "            \n",
    "            duplicates = set()\n",
    "            for i in range(len(cos_sim)):\n",
    "                if i not in duplicates:\n",
    "                    similar_indices = [j for j in range(i+1, len(cos_sim)) \n",
    "                                     if cos_sim[i][j] >= 0.85]\n",
    "                    duplicates.update(similar_indices)\n",
    "            \n",
    "            df = df.drop(index=list(duplicates)).copy()\n",
    "            print(f\"相似去重后: {len(df)} 条\")\n",
    "        except Exception as e:\n",
    "            print(f\"相似去重时出错: {str(e)}，跳过此步骤\")\n",
    "    elif len(df) > 500:\n",
    "        print(\"数据集较大(>500条)，跳过相似度去重\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 去重结果验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 正在处理数据集: 阿里健康大药房 ===\n",
      "原始记录数: 1044\n",
      "完全去重后: 936 条\n",
      "无效过滤后: 798 条\n",
      "数据集较大(>500条)，跳过相似度去重\n",
      "已保存到: analysis_results/阿里健康大药房_cleaned.xlsx\n",
      "\n",
      "\n",
      "=== 正在处理数据集: 福士德 ===\n",
      "原始记录数: 391\n",
      "完全去重后: 366 条\n",
      "无效过滤后: 315 条\n",
      "相似去重后: 310 条\n",
      "已保存到: analysis_results/福士德_cleaned.xlsx\n",
      "\n",
      "\n",
      "=== 正在处理数据集: 耀典 ===\n",
      "原始记录数: 319\n",
      "完全去重后: 303 条\n",
      "无效过滤后: 232 条\n",
      "相似去重后: 230 条\n",
      "已保存到: analysis_results/耀典_cleaned.xlsx\n",
      "\n",
      "\n",
      "=== 正在处理数据集: hfine ===\n",
      "原始记录数: 423\n",
      "完全去重后: 414 条\n",
      "无效过滤后: 360 条\n",
      "相似去重后: 358 条\n",
      "已保存到: analysis_results/hfine_cleaned.xlsx\n",
      "\n",
      "\n",
      "=== 去重汇总 ===\n",
      "阿里健康大药房: 1044 → 798 (减少23.6%)\n",
      "福士德: 391 → 310 (减少20.7%)\n",
      "耀典: 319 → 230 (减少27.9%)\n",
      "hfine: 423 → 358 (减少15.4%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\avawa\\AppData\\Local\\Temp\\ipykernel_4700\\1241546093.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['字段1'] = df['字段1'].astype(str)\n",
      "C:\\Users\\avawa\\AppData\\Local\\Temp\\ipykernel_4700\\1241546093.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['字段1'] = df['字段1'].astype(str)\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '说', '达', '非'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\avawa\\AppData\\Local\\Temp\\ipykernel_4700\\1241546093.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['字段1'] = df['字段1'].astype(str)\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '说', '达', '非'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\avawa\\AppData\\Local\\Temp\\ipykernel_4700\\1241546093.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['字段1'] = df['字段1'].astype(str)\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\avawa\\.conda\\envs\\statmodel\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['傥', '兼', '前', '唷', '啪', '啷', '喔', '天', '始', '漫', '然', '特', '竟', '莫', '见', '设', '说', '达', '非'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 创建输出目录\n",
    "os.makedirs('analysis_results', exist_ok=True)\n",
    "\n",
    "# 对每个数据集执行去重\n",
    "cleaned_datasets = {}\n",
    "for name, df in all_datasets.items():\n",
    "    try:\n",
    "        # 确保数据是DataFrame且包含'字段1'\n",
    "        if not isinstance(df, pd.DataFrame) or '字段1' not in df.columns:\n",
    "            print(f\"数据集 {name} 格式不正确，跳过\")\n",
    "            continue\n",
    "            \n",
    "        # 执行去重\n",
    "        cleaned_df = deduplicate_dataset(df.copy(), name)\n",
    "        cleaned_datasets[name] = cleaned_df\n",
    "        \n",
    "        # 保存结果\n",
    "        output_path = f'analysis_results/{name}_cleaned.xlsx'\n",
    "        cleaned_df.to_excel(output_path, index=False)\n",
    "        print(f\"已保存到: {output_path}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"处理数据集 {name} 时出错: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# 汇总统计\n",
    "if cleaned_datasets:\n",
    "    print(\"\\n=== 去重汇总 ===\")\n",
    "    for name in all_datasets.keys():\n",
    "        if name in cleaned_datasets:\n",
    "            orig_count = len(all_datasets[name])\n",
    "            cleaned_count = len(cleaned_datasets[name])\n",
    "            reduction = (orig_count - cleaned_count) / orig_count if orig_count > 0 else 0\n",
    "            print(f\"{name}: {orig_count} → {cleaned_count} (减少{reduction:.1%})\")\n",
    "else:\n",
    "    print(\"没有执行任何去重操作\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用数据集: ['阿里健康大药房', '福士德', '耀典', 'hfine']\n",
      "\n",
      "数据集: 福士德\n",
      "删除记录数: 81\n",
      "\n",
      "被删除的评论示例:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "字段1",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "89769ac1-5276-49b1-bef5-65656d1cc69e",
       "rows": [
        [
         "85",
         "非常好"
        ],
        [
         "76",
         "非常好用"
        ],
        [
         "81",
         "做工精致"
        ],
        [
         "144",
         "很轻便"
        ],
        [
         "104",
         "非常好"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>字段1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>非常好</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>非常好用</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>做工精致</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>很轻便</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>非常好</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      字段1\n",
       "85    非常好\n",
       "76   非常好用\n",
       "81   做工精致\n",
       "144   很轻便\n",
       "104   非常好"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_deleted_examples(original_name):\n",
    "    \"\"\"显示被删除的评论示例\"\"\"\n",
    "    if original_name not in all_datasets or original_name not in cleaned_datasets:\n",
    "        print(f\"数据集 {original_name} 不存在\")\n",
    "        return\n",
    "    \n",
    "    original = all_datasets[original_name]\n",
    "    cleaned = cleaned_datasets[original_name]\n",
    "    \n",
    "    # 找出被删除的记录\n",
    "    merged = original.merge(\n",
    "        cleaned, \n",
    "        how='left', \n",
    "        indicator=True, \n",
    "        on=list(original.columns)\n",
    "    )\n",
    "    deleted = merged[merged['_merge'] == 'left_only']\n",
    "    \n",
    "    print(f\"\\n数据集: {original_name}\")\n",
    "    print(f\"删除记录数: {len(deleted)}\")\n",
    "    \n",
    "    if not deleted.empty:\n",
    "        print(\"\\n被删除的评论示例:\")\n",
    "        display(deleted.sample(min(5, len(deleted)))[['字段1']])\n",
    "    else:\n",
    "        print(\"\\n没有删除任何评论\")\n",
    "\n",
    "# 交互式验证\n",
    "if cleaned_datasets:\n",
    "    print(\"可用数据集:\", list(cleaned_datasets.keys()))\n",
    "    dataset_to_check = input(\"输入要验证的数据集名称: \")\n",
    "    show_deleted_examples(dataset_to_check)\n",
    "else:\n",
    "    print(\"没有可验证的去重结果\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (stat)",
   "language": "python",
   "name": "statmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
