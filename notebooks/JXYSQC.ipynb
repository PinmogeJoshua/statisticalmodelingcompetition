{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 导入库和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# 设置显示选项\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 定义机械压缩去词函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mechanical_compression(text, max_repeat=3):\n",
    "    \"\"\"\n",
    "    机械压缩去词处理\n",
    "    :param text: 输入文本\n",
    "    :param max_repeat: 允许的最大重复次数\n",
    "    :return: 处理后的文本\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) < 2:\n",
    "        return text\n",
    "    \n",
    "    # 处理连续重复字符\n",
    "    def process_repeats(s):\n",
    "        # 匹配2个及以上连续重复的中文字符或英文单词\n",
    "        pattern = re.compile(r'([\\u4e00-\\u9fa5])\\1{1,}|([a-zA-Z]+\\s?)\\2{1,}')\n",
    "        while True:\n",
    "            match = pattern.search(s)\n",
    "            if not match:\n",
    "                break\n",
    "            matched = match.group()\n",
    "            # 保留最多max_repeat次重复\n",
    "            if len(matched) > len(match.group(1) or match.group(2)) * max_repeat:\n",
    "                replacement = (match.group(1) or match.group(2)) * min(max_repeat, len(matched))\n",
    "                s = s[:match.start()] + replacement + s[match.end():]\n",
    "            else:\n",
    "                break\n",
    "        return s\n",
    "    \n",
    "    # 处理开头和结尾的重复模式\n",
    "    def process_edges(s):\n",
    "        # 处理开头重复\n",
    "        start_pattern = re.compile(r'^([\\u4e00-\\u9fa5]{1,3})\\1+')\n",
    "        match = start_pattern.match(s)\n",
    "        if match:\n",
    "            matched = match.group()\n",
    "            replacement = match.group(1) * min(max_repeat, len(matched)//len(match.group(1)))\n",
    "            s = replacement + s[match.end():]\n",
    "        \n",
    "        # 处理结尾重复\n",
    "        end_pattern = re.compile(r'([\\u4e00-\\u9fa5]{1,3})\\1+$')\n",
    "        match = end_pattern.search(s)\n",
    "        if match:\n",
    "            matched = match.group()\n",
    "            replacement = match.group(1) * min(max_repeat, len(matched)//len(match.group(1)))\n",
    "            s = s[:match.start()] + replacement\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    text = process_repeats(text)\n",
    "    text = process_edges(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 定义处理单个数据集的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(input_path, output_path, max_repeat=3):\n",
    "    \"\"\"\n",
    "    对单个数据集进行机械压缩去词处理，并保存结果。\n",
    "    :param input_path: 输入数据集路径（xlsx 文件）\n",
    "    :param output_path: 输出数据集路径（xlsx 文件）\n",
    "    :param max_repeat: 允许的最大重复次数\n",
    "    \"\"\"\n",
    "    # 加载数据集\n",
    "    print(f\"正在处理数据集：{input_path}\")\n",
    "    df = pd.read_excel(input_path)\n",
    "\n",
    "    # 检查是否有评论列\n",
    "    if '字段1' not in df.columns:  # 假设评论列名为 '字段1'\n",
    "        print(f\"数据集 {input_path} 中未找到 '字段1' 列，请检查数据格式。\")\n",
    "        return\n",
    "\n",
    "    # 对评论列应用机械压缩去词\n",
    "    df['compressed_comment'] = df['字段1'].apply(lambda x: mechanical_compression(x, max_repeat=max_repeat))\n",
    "\n",
    "    # 保存处理后的数据集\n",
    "    df.to_excel(output_path, index=False)  # 移除了 encoding 参数\n",
    "    print(f\"处理完成，结果已保存到：{output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 处理四个数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理数据集：text_prossessed\\阿里健康大药房_cleaned.xlsx\n",
      "处理完成，结果已保存到：compressed_results/阿里健康大药房_compressed.xlsx\n",
      "正在处理数据集：text_prossessed\\福士德_cleaned.xlsx\n",
      "处理完成，结果已保存到：compressed_results/福士德_compressed.xlsx\n",
      "正在处理数据集：text_prossessed\\耀典_cleaned.xlsx\n",
      "处理完成，结果已保存到：compressed_results/耀典_compressed.xlsx\n",
      "正在处理数据集：text_prossessed\\hfine_cleaned.xlsx\n",
      "处理完成，结果已保存到：compressed_results/hfine_compressed.xlsx\n",
      "所有数据集处理完成！结果保存在文件夹：compressed_results\n"
     ]
    }
   ],
   "source": [
    "# 定义输出文件夹\n",
    "output_folder = \"compressed_results\"\n",
    "\n",
    "# 如果文件夹不存在，则创建\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 定义四个数据集的路径\n",
    "datasets = [\n",
    "    {\"input\": \"text_prossessed\\阿里健康大药房_cleaned.xlsx\", \"output\": f\"{output_folder}/阿里健康大药房_compressed.xlsx\"},\n",
    "    {\"input\": \"text_prossessed\\福士德_cleaned.xlsx\", \"output\": f\"{output_folder}/福士德_compressed.xlsx\"},\n",
    "    {\"input\": \"text_prossessed\\耀典_cleaned.xlsx\", \"output\": f\"{output_folder}/耀典_compressed.xlsx\"},\n",
    "    {\"input\": \"text_prossessed\\hfine_cleaned.xlsx\", \"output\": f\"{output_folder}/hfine_compressed.xlsx\"}\n",
    "]\n",
    "\n",
    "# 对每个数据集进行处理\n",
    "for dataset in datasets:\n",
    "    process_dataset(dataset[\"input\"], dataset[\"output\"])\n",
    "\n",
    "print(f\"所有数据集处理完成！结果保存在文件夹：{output_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9 (stat)",
   "language": "python",
   "name": "statmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
